---
title: "Assignment-2"
output: html_notebook
---

Step 1: Installing necessary packages:

```{r}
#install.packages("dplyr")
library(dplyr)
#install.packages("ISLR")
library(ISLR)
#install.packages("caret")
library(caret)
#install.packages("MASS")
library(MASS)
#install.packages("FNN")
library(FNN)
#install.packages("class")
library(class)
#install.packages("gmodels")
library(gmodels)
#install.packages("e1071")
library(e1071)
```

Step 2: Reading from the file

```{r}
Original_Data <- read.csv("C:/Users/BrandRely/Documents/Datasets/UniversalBank.csv")
head(Original_Data)
summary(Original_Data)
```

Step 3: Removing Id & Zip

```{r}
Original_Data <- Original_Data[,c(-1,-5)]
summary(Original_Data)
```

Step 4: Transforming Education variable as it has more than 2 categories

```{r}
test_data_1 <- Original_Data$Education
test_factor <- factor(test_data_1, label =c("E1","E2","E3"))
Original_Data[["Education"]] <- test_factor
levels(Original_Data$Education)
```

Step 5: Using dummyVars on Education variable & removing Education column

```{r}
dummy_model <- dummyVars("~Education", data = Original_Data)
a <- predict(dummy_model,newdata=Original_Data)
Original_Data <- cbind(Original_Data, a)
Original_Data <- Original_Data[, c(-6)]
Original_Data
```

Step 6: Normalizing the data

```{r}
norm_model <-preProcess(Original_Data[, c(1,2,3,4,5,6)], method = c("center", "scale"))
default_normalized <- predict(norm_model,Original_Data)
summary(default_normalized)
```

Step 7: Partitioning data in Training & Validation sets as 60% & 40% respectively:

```{r}
set.seed(15)
partition <- createDataPartition(default_normalized$Personal.Loan, p=0.6, list=FALSE)
training_data <- default_normalized[partition,]
validation_data <- default_normalized[-partition,]
train_predictors <- training_data[,-7]
validation_Predictors <- validation_data[,-7]
train_labels <- training_data[,7]
validation_labels <- validation_data[,7] 
train_data_factored <- as.factor(train_labels)
valid_data_factored <- as.factor(validation_labels)
```

Step 8: Using KNN and taking K = 1

```{r}
predicted_test_labels_1 <- knn(train_predictors, validation_Predictors, cl=train_data_factored, k=1, prob = TRUE)
summary(predicted_test_labels_1)
```

Step 9: Putting the values given in the question for the variables

```{r}
input_data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
Predicted_Test_label2 <- knn(train_predictors, input_data , cl=train_labels, k=1, prob = TRUE)
Predicted_Test_label2
summary(Predicted_Test_label2)
```

Step 10: Balancing K 

```{r}
validation_labels2 <- as.factor(validation_labels)
accuracy.df <- data.frame(k = seq(1, 100, 1), accuracy = rep(0, 100))
```

Step 11: Computing KNN for different K on Validation

```{r}
for(i in 1:15) {
  predicted_test_labels_2 <- knn(train_predictors, validation_Predictors, cl=train_data_factored, k=i)
  accuracy.df[i, 2] <- confusionMatrix(predicted_test_labels_2, valid_data_factored)$overall[1] 
}
accuracy.df
final_k <- accuracy.df[which.max(accuracy.df$accuracy),]
final_k
```

Step 12: Creating the Confusion Matrix

```{r}
Predicted_Test_label5 <- knn(train_predictors, validation_Predictors , cl=train_labels, k=3, prob = TRUE)
CrossTable(x=valid_data_factored, y=Predicted_Test_label5, prop.chisq = FALSE)
```

Step 13: Solving for the given values for variables

```{r}
input_data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
Predicted_Test_label4 <- knn(train_predictors,input_data , cl=train_data_factored, k=3, prob = TRUE)
Predicted_Test_label4
str(Predicted_Test_label4)

```

Step 14: Repartition of Data (First repeating the process from Step 2 - Step 6)

```{r}

Original_Data <- read.csv("C:/Users/BrandRely/Documents/Datasets/UniversalBank.csv")
head(Original_Data)
summary(Original_Data)


Original_Data <- Original_Data[,c(-1,-5)]
summary(Original_Data)

test_data_1 <- Original_Data$Education
test_factor <- factor(test_data_1, label =c("E1","E2","E3"))
Original_Data[["Education"]] <- test_factor
levels(Original_Data$Education)


dummy_model <- dummyVars("~Education", data = Original_Data)
a <- predict(dummy_model,newdata=Original_Data)
Original_Data <- cbind(Original_Data, a)
Original_Data <- Original_Data[, c(-6)]
Original_Data

norm_model <-preProcess(Original_Data[, c(1,2,3,4,5,6)], method = c("center", "scale"))
default_normalized <- predict(norm_model,Original_Data)
summary(default_normalized)
```

Step 15: Partitioning data in Training, Validation & Test sets as 50%, 30% & 20% respectively:

```{r}
set.seed(15)

#Test Data is reserved

test_index <- createDataPartition(default_normalized$Personal.Loan,p=0.2, list=FALSE) 
Test_Data <- Original_Data[test_index,]

#The rest will be divided between Training & Validation

Rest_Data <- Original_Data[-test_index,]

#Train Data
train_index = createDataPartition(Rest_Data$Personal.Loan,p=0.3, list=FALSE) 
Train_Data = Rest_Data[-train_index,]

#Validation data is rest
Validation_Data = Rest_Data[train_index,] 
summary(Train_Data)

train_predictors<-Train_Data[,-7]
validation_Predictors<- Validation_Data[,-7]
valid_data_factored <- as.factor(validation_labels)
train_labels <- Train_Data[,7]
train_data_factored <- as.factor(train_labels)
validation_labels  <-Validation_Data[,7]
test_labels <- Test_Data[,7]
test.f <- as.factor(test_labels)
test_predictors <- Test_Data[,-7]
predicted_test_labels_1 <- knn(train_predictors, test_predictors, cl=train_data_factored, k=3, prob = TRUE)
accuracy.df <- data.frame(k = seq(1, 25, 1), accuracy = rep(0, 25))
```

Step 16: Computing KNN for different K on Validation

```{r}
for(i in 1:15) {
  predicted_test_labels_2 <- knn(train_predictors, validation_Predictors, cl=train_data_factored, k=i)
  accuracy.df[i, 2] <- confusionMatrix(predicted_test_labels_2, valid_data_factored)$overall[1] 
}


accuracy.df
final_k <- accuracy.df[which.max(accuracy.df$accuracy),]
final_k

```

Step 17: Creating the Confusion Matrix

```{r}
predicted_test_labels_3 <- knn(validation_Predictors, test_predictors, cl=valid_data_factored, k=3, prob = TRUE)
CrossTable(x=test_labels, y=predicted_test_labels_3, prop.chisq = FALSE)
```




