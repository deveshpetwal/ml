---
title: "Dev Assignment 2 Submission"
output: html_document
---

Step 1: Installing necessary packages:

```{r}
#install.packages("dplyr")
library(dplyr)
#install.packages("ISLR")
library(ISLR)
#install.packages("caret")
library(caret)
#install.packages("MASS")
library(MASS)
#install.packages("FNN")
library(FNN)
#install.packages("class")
library(class)
#install.packages("gmodels")
library(gmodels)
#install.packages("e1071")
library(e1071)
```

Step 2: Reading from the file

```{r}
Original_Data <- read.csv("C:/Users/BrandRely/Documents/Datasets/UniversalBank.csv")
head(Original_Data)
summary(Original_Data)
```

Step 3: Removing Id & Zip

```{r}
Original_Data <- Original_Data[,c(-1,-5)]
summary(Original_Data)
```

Step 4: Transforming Education variable as it has more than 2 categories

```{r}
transform_data <- Original_Data$Education
test_factor <- factor(transform_data, label =c("E1","E2","E3"))
Original_Data[["Education"]] <- test_factor
levels(Original_Data$Education)
```

Step 5: Using dummyVars on Education variable & removing Education column

```{r}
dummy_model <- dummyVars("~Education", data = Original_Data)
a <- predict(dummy_model,newdata = Original_Data)
Original_Data <- cbind(Original_Data, a)
Original_Data <- Original_Data[, c(-6)]
head(Original_Data)
```

Step 6: Partitioning data in Training & Validation sets as 60% & 40% respectively:

```{r}
set.seed(100)
partition <- createDataPartition(Original_Data$Personal.Loan, p=0.6, list=FALSE)
training_data <- Original_Data[partition,]
validation_data <- Original_Data[-partition,]
```

Step 7: Normalization of Data 

```{r}
norm_data <- preProcess(training_data, method = c ("center","scale"))
train.norm.df <- predict(norm_data, training_data)
valid.norm.df <- predict(norm_data, validation_data)
total.norm.df <- predict(norm_data, Original_Data)

train_predictors <- train.norm.df[,-7]
validation_Predictors <- valid.norm.df[,-7]
train_labels <- train.norm.df[,7]
validation_labels <- valid.norm.df[,7] 
train_data_factored <- as.factor(train_labels)
valid_data_factored <- as.factor(validation_labels)

Original_Data <- total.norm.df[,-7]
```


Step 8: Using KNN and taking K = 1

```{r}
model <- knn(train_predictors, validation_Predictors, cl=train_labels, k=1, prob = TRUE)
summary(model)
```

Step 9: Putting the values given in the question for the variables (Question-1)

```{r}
input_data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
Predicted_Test_label_1 <- knn(train_predictors, input_data , cl=train_labels, k=1, prob = TRUE)
Predicted_Test_label_1
#Therefore the Loan is accepted
```

Step 10: Computing for Choice of k that balances between overfitting and ignoring the predictor information (Question-2)

```{r}
accuracy.df <- data.frame(k = seq(1, 100, 1), accuracy = rep(0, 100))

for(i in 1:15) {
  predicted_test_labels_2 <- knn(train_predictors, validation_Predictors, cl=train_data_factored, k=i)
  accuracy.df[i, 2] <- confusionMatrix(predicted_test_labels_2, valid_data_factored)$overall[1] 
}
accuracy.df
final_k <- accuracy.df[which.max(accuracy.df$accuracy),]
final_k
# So choice of k: 3
```

Step 11: Creating the Confusion Matrix (Question-3)

```{r}
Predicted_Test_label3 <- knn(train_predictors, validation_Predictors , cl=train_data_factored, k=3, prob = TRUE)
CrossTable(x=valid_data_factored, y=Predicted_Test_label3, prop.chisq = FALSE)
```

Step 12: Solving for the given values for variables with best K = 3 (Question-4)

```{r}
input_data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
Predicted_Test_label4 <- knn(train_predictors,input_data , cl=train_labels, k=3, prob = TRUE)
Predicted_Test_label4
#Therefore the Loan is accepted
```

Step 13: Repartition of Data (First repeating the process from Step 2 - Step 5 (Question-5)

```{r}

Original_Data <- read.csv("C:/Users/BrandRely/Documents/Datasets/UniversalBank.csv")
head(Original_Data)
summary(Original_Data)


Original_Data <- Original_Data[,c(-1,-5)]
summary(Original_Data)

transform_data <- Original_Data$Education
test_factor <- factor(transform_data, label =c("E1","E2","E3"))
Original_Data[["Education"]] <- test_factor
levels(Original_Data$Education)


dummy_model <- dummyVars("~Education", data = Original_Data)
a <- predict(dummy_model,newdata=Original_Data)
Original_Data <- cbind(Original_Data, a)
Original_Data <- Original_Data[, c(-6)]
head(Original_Data)

```

Step 14: Partitioning data in Training, Validation & Test sets as 50%, 30% & 20% respectively:

```{r}
set.seed(100)

#Test Data is reserved

test_index <- createDataPartition(Original_Data$Personal.Loan,p=0.2, list=FALSE) 
Test_Data <- Original_Data[test_index,]
Rest_Data <- Original_Data[-test_index,]

#The rest will be divided between Training & Validation

#Train Data
train_index = createDataPartition(Rest_Data$Personal.Loan,p=0.625, list=FALSE) 
Train_Data = Rest_Data[train_index,]

#Validation data is rest
Validation_Data = Rest_Data[-train_index,] 
summary(Train_Data)

#Normilization of data

normalization<-preProcess(Train_Data, method = c("center","scale"))
Train_Norm<-predict(normalization,Train_Data)
Test_Norm<-predict(normalization,Test_Data)
Validation_Norm<-predict(normalization,Validation_Data)
All_Data_Norm<-predict(normalization,Original_Data)

Train_Predictors<-Train_Norm[,-7]
Test_predictors <- Test_Norm[,-7]
validation_Predictors<- Validation_Norm[,-7]
All_Data_Predictors<- All_Data_Norm[,-7]
Train_labels <-factor(Train_Data[,7], levels=c(0,1))
Validation_labels  <-factor(Validation_Data[,7], levels=c(0,1))
Test_labels <- factor(Test_Data[,7], levels=c(0,1))
All_Data_labels <- factor(Original_Data[,7], levels=c(0,1))
     
predicted_Test_labels <- knn(Train_Predictors, Test_predictors, cl=Train_labels, k=3, prob = TRUE)

predicted_valid_labels <- knn(train_predictors, validation_Predictors, cl=train_labels, k=3, prob = TRUE)

```


Step 15: Creating the Confusion Matrix

```{r}

CrossTable(x=Validation_labels, y=predicted_valid_labels, prop.chisq = FALSE)

CrossTable(x=Test_labels, y=predicted_Test_labels, prop.chisq = FALSE)

#Accuracy of model is reduced as data partition second time reduced the amount of data given to train our model. 
```




